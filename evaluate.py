# evaluate.py
import pandas as pd
from baseline import simple_rag_answer # ì²­ì½”ë„ˆ
from graph_agent import get_graph_app   # í™ì½”ë„ˆ
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

# 1. ì‹¬íŒ ëª¨ë¸ (GPT-4)
judge_llm = ChatOpenAI(model="gpt-4", temperature=0)

# 2. í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ (ê²€ì¦í•˜ê³  ì‹¶ì€ ì§ˆë¬¸ë“¤)
test_questions = [
    "5ê°œì›” ì•„ê¸° ì´ìœ ì‹ ìŠ¤ì¼€ì¤„ ì§œì¤˜",           # ë³µí•© ì§ˆë¬¸
    "ì•„ê¸°ê°€ ë°¤ì— ê³„ì† ê¹¨ëŠ”ë° ìˆ˜ë©´ êµìœ¡ ì–´ë–»ê²Œ í•´?", # ìˆ˜ë©´ ì „ë¬¸ ì§€ì‹ í•„ìš”
    "ì² ë¶„ ë¶€ì¡±í•˜ë©´ ë­˜ ë¨¹ì—¬ì•¼ í•´?",             # ì˜ì–‘ ì „ë¬¸ ì§€ì‹ í•„ìš”
    "ëŒ ì§€ë‚œ ì•„ê¸° ìš°ìœ  ì–¼ë§ˆë‚˜ ë§ˆì…”?",           # êµ¬ì²´ì  ìˆ˜ì¹˜ í•„ìš”
]

# 3. ì±„ì  í”„ë¡¬í”„íŠ¸ (ê°€ì¥ ì¤‘ìš”!)
judge_prompt = ChatPromptTemplate.from_template("""
ë„ˆëŠ” AI ë‹µë³€ í‰ê°€ìë‹¤. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ë‘ ê°€ì§€ ë‹µë³€(A, B)ì´ ìˆë‹¤.
ë” 'ì „ë¬¸ì 'ì´ê³ , 'êµ¬ì²´ì 'ì´ë©°, 'ë„ì›€ì´ ë˜ëŠ”' ë‹µë³€ì„ ì„ íƒí•˜ê³  ì´ìœ ë¥¼ ì„¤ëª…í•˜ë¼.

[ì§ˆë¬¸]: {question}

[ë‹µë³€ A (Single Agent)]:
{answer_a}

[ë‹µë³€ B (Multi-Agent)]:
{answer_b}

í‰ê°€ ê¸°ì¤€:
1. ì •í™•ì„±: ê²€ìƒ‰ëœ ì •ë³´ì— ê¸°ë°˜í–ˆëŠ”ê°€?
2. ì „ë¬¸ì„±: ì „ë¬¸ê°€ë‹¤ìš´ ì–´ì¡°ì™€ ê¹Šì´ê°€ ìˆëŠ”ê°€?
3. êµ¬ì¡°: ì½ê¸° í¸í•˜ê²Œ ì •ë¦¬ë˜ì—ˆëŠ”ê°€?

ê²°ê³¼ í˜•ì‹:
- ìŠ¹ì: (A ë˜ëŠ” B)
- ì´ìœ : (í•œ ì¤„ ìš”ì•½)
""")

# 4. í‰ê°€ ì‹¤í–‰ í•¨ìˆ˜
def run_evaluation():
    results = []
    app = get_graph_app() # Multi-Agent
    
    config = {"configurable": {"thread_id": "eval_user"}}
    eval_chain = judge_prompt | judge_llm
    print("ğŸš€ í‰ê°€ ì‹œì‘...\n")
    
    for q in test_questions:
        print(f"Testing: {q}")
        
        # A: Baseline ì‹¤í–‰
        try:
            ans_a = simple_rag_answer(q)
        except Exception as e:
            ans_a = f"Error: {e}"
        
        # B: Multi-Agent ì‹¤í–‰
        response = app.invoke({"messages": [{"role": "user", "content": q}]}, config=config)
        ans_b = response["messages"][-1].content
        
        # ì‹¬íŒ ì±„ì 
        eval_result_msg = eval_chain.invoke({
            "question": q,
            "answer_a": ans_a,
            "answer_b": ans_b
        })

        eval_content = eval_result_msg.content # ê²°ê³¼ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        print(f"   -> ì‹¬íŒ íŒì •: {eval_content[:50]}...") # ë¡œê·¸ ì‚´ì§ ì¶œë ¥
        
        results.append({
            "Question": q,
            "Winner": "Multi-Agent" if "ìŠ¹ì: B" in eval_content or "ìŠ¹ì:B" in eval_content else "Single Agent",
            "Evaluation": eval_content
        })

    # 5. ê²°ê³¼ ì¶œë ¥
    df = pd.DataFrame(results)
    print("\nğŸ“Š ìµœì¢… í‰ê°€ ê²°ê³¼:")
    print(df[["Question", "Winner"]])
    
    # ì—‘ì…€ë¡œ ì €ì¥ (ì¦ê±° ìë£Œ)
    df.to_excel("evaluation_results.xlsx")
    print("\nâœ… ê²°ê³¼ê°€ 'evaluation_results.xlsx'ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    run_evaluation()