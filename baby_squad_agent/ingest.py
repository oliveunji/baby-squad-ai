import os
from dotenv import load_dotenv
from langchain_community.document_loaders import TextLoader, PyPDFLoader, DirectoryLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_chroma import Chroma

# 1. í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ (API Key í•„ìš”)
load_dotenv()

# API Key í™•ì¸
if not os.getenv("GOOGLE_API_KEY"):
    print("âŒ Error: .env íŒŒì¼ì— GOOGLE_API_KEYê°€ ì—†ìŠµë‹ˆë‹¤.")
    exit()

def ingest_data():
    print("ğŸš€ BabySquad ì§€ì‹ ì£¼ì…(ê³µë¶€) ì‹œì‘...")

    # ---------------------------------------------------------
    # [1] ë¬¸ì„œ ë¡œë“œ (Load Documents)
    # data í´ë”ì— ìˆëŠ” txt, pdf íŒŒì¼ì„ ëª¨ë‘ ì½ì–´ì˜µë‹ˆë‹¤.
    # ---------------------------------------------------------
    data_path = "./data"
    if not os.path.exists(data_path):
        os.makedirs(data_path)
        print(f"âš ï¸ '{data_path}' í´ë”ê°€ ì—†ì–´ì„œ ìƒì„±í–ˆìŠµë‹ˆë‹¤. í•™ìŠµí•  íŒŒì¼ì„ ë„£ì–´ì£¼ì„¸ìš”!")
        return

    # í…ìŠ¤íŠ¸ íŒŒì¼ ë¡œë”
    loader = DirectoryLoader(data_path, glob="**/*.txt", loader_cls=TextLoader)
    documents = loader.load()
    
    # PDF íŒŒì¼ì´ ìˆë‹¤ë©´ ì•„ë˜ ì£¼ì„ í•´ì œ
    pdf_loader = DirectoryLoader(data_path, glob="**/*.pdf", loader_cls=PyPDFLoader)
    documents.extend(pdf_loader.load())

    if not documents:
        print("ğŸ“‚ data í´ë”ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. í•™ìŠµí•  í…ìŠ¤íŠ¸ íŒŒì¼(.txt)ì„ ë„£ì–´ì£¼ì„¸ìš”.")
        return

    print(f"ğŸ“š ì´ {len(documents)}ê°œì˜ ë¬¸ì„œë¥¼ ì½ì–´ì™”ìŠµë‹ˆë‹¤.")

    # ---------------------------------------------------------
    # [2] ë¬¸ì„œ ë¶„í•  (Split Documents)
    # ì±…ì„ í•œ ë²ˆì— ë‹¤ ì™¸ìš¸ ìˆ˜ ì—†ìœ¼ë‹ˆ, ë¬¸ë‹¨ ë‹¨ìœ„ë¡œ ìª¼ê°­ë‹ˆë‹¤.
    # ---------------------------------------------------------
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,  # 1000ì ë‹¨ìœ„ë¡œ ìë¦„
        chunk_overlap=200 # ë¬¸ë§¥ì´ ëŠê¸°ì§€ ì•Šê²Œ 200ìì”© ê²¹ì¹˜ê²Œ ìë¦„
    )
    chunks = text_splitter.split_documents(documents)
    print(f"âœ‚ï¸  ë¬¸ì„œë¥¼ {len(chunks)}ê°œì˜ ì¡°ê°(Chunk)ìœ¼ë¡œ ì˜ëìŠµë‹ˆë‹¤.")

    # ---------------------------------------------------------
    # [3] ì„ë² ë”© ë° DB ì €ì¥ (Embed & Store)
    # í…ìŠ¤íŠ¸ë¥¼ AIê°€ ì´í•´í•˜ëŠ” ìˆ«ì(Vector)ë¡œ ë°”ê¿”ì„œ ChromaDBì— ì €ì¥í•©ë‹ˆë‹¤.
    # ---------------------------------------------------------
    embeddings = GoogleGenerativeAIEmbeddings(model="models/text-embedding-004")
    
    # DB ì €ì¥ ê²½ë¡œ
    persist_directory = "./chroma_db"
    
    print("ğŸ’¾ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ ì¤‘... (ì‹œê°„ì´ ì¡°ê¸ˆ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
    vector_store = Chroma.from_documents(
        documents=chunks,
        embedding=embeddings,
        persist_directory=persist_directory,
        collection_name="baby_knowledge" # ë°ì´í„°ë² ì´ìŠ¤ ì´ë¦„
    )
    
    print(f"âœ… í•™ìŠµ ì™„ë£Œ! ë°ì´í„°ê°€ '{persist_directory}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    ingest_data()